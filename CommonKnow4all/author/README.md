# Commonsense Knowledge Construction and Application Literature 
This repository is maintained by [Tongtong Wu](https://wutong8023.site). Please don't hesitate to send me an email to collaborate or fix some entries (wutong8023 AT gmail.com). 
The automation script of this repo is powered by [Auto-Bibfile](https://github.com/wutong8023/Auto-Bibfile.git).

You can directly use our bibtex.bib in overleaf with this [link](https://www.overleaf.com/read/rgscdxhxbwhp).

This page categorizes the literature by their **Authors** who contributed at least 2 papers in this area..

## Outline 
- [![](https://img.shields.io/badge/Hyperlink-blue)](https://github.com/wutong8023/Awesome_Commonsense_Knowledge/blob/main/CommonKnow4all/author/README.md#hyperlink)
- [![](https://img.shields.io/badge/Yejin_Choi-6-blue)](https://github.com/wutong8023/Awesome_Commonsense_Knowledge/blob/main/CommonKnow4all/author/README.md#yejin-choi)
- [![](https://img.shields.io/badge/Hannah_Rashkin-5-blue)](https://github.com/wutong8023/Awesome_Commonsense_Knowledge/blob/main/CommonKnow4all/author/README.md#hannah-rashkin)
- [![](https://img.shields.io/badge/Maarten_Sap-4-blue)](https://github.com/wutong8023/Awesome_Commonsense_Knowledge/blob/main/CommonKnow4all/author/README.md#maarten-sap)
- [![](https://img.shields.io/badge/Xiang_Ren-3-blue)](https://github.com/wutong8023/Awesome_Commonsense_Knowledge/blob/main/CommonKnow4all/author/README.md#xiang-ren)
- [![](https://img.shields.io/badge/Ronan_Le_Bras-3-blue)](https://github.com/wutong8023/Awesome_Commonsense_Knowledge/blob/main/CommonKnow4all/author/README.md#ronan-le-bras)
- [![](https://img.shields.io/badge/Jay_Pujara-2-blue)](https://github.com/wutong8023/Awesome_Commonsense_Knowledge/blob/main/CommonKnow4all/author/README.md#jay-pujara)
- [![](https://img.shields.io/badge/Bill_Yuchen_Lin-2-blue)](https://github.com/wutong8023/Awesome_Commonsense_Knowledge/blob/main/CommonKnow4all/author/README.md#bill-yuchen-lin)
- [![](https://img.shields.io/badge/Pei_Zhou-2-blue)](https://github.com/wutong8023/Awesome_Commonsense_Knowledge/blob/main/CommonKnow4all/author/README.md#pei-zhou)
- [![](https://img.shields.io/badge/Minlie_Huang-2-blue)](https://github.com/wutong8023/Awesome_Commonsense_Knowledge/blob/main/CommonKnow4all/author/README.md#minlie-huang)
- [![](https://img.shields.io/badge/Hao_Zhou-2-blue)](https://github.com/wutong8023/Awesome_Commonsense_Knowledge/blob/main/CommonKnow4all/author/README.md#hao-zhou)
- [![](https://img.shields.io/badge/Tom_Young-2-blue)](https://github.com/wutong8023/Awesome_Commonsense_Knowledge/blob/main/CommonKnow4all/author/README.md#tom-young)
- [![](https://img.shields.io/badge/Samuel_R._Bowman-2-blue)](https://github.com/wutong8023/Awesome_Commonsense_Knowledge/blob/main/CommonKnow4all/author/README.md#samuel-r.-bowman)
- [![](https://img.shields.io/badge/Omer_Levy-2-blue)](https://github.com/wutong8023/Awesome_Commonsense_Knowledge/blob/main/CommonKnow4all/author/README.md#omer-levy)
- [![](https://img.shields.io/badge/Felix_Hill-2-blue)](https://github.com/wutong8023/Awesome_Commonsense_Knowledge/blob/main/CommonKnow4all/author/README.md#felix-hill)
- [![](https://img.shields.io/badge/Julian_Michael-2-blue)](https://github.com/wutong8023/Awesome_Commonsense_Knowledge/blob/main/CommonKnow4all/author/README.md#julian-michael)
- [![](https://img.shields.io/badge/Amanpreet_Singh-2-blue)](https://github.com/wutong8023/Awesome_Commonsense_Knowledge/blob/main/CommonKnow4all/author/README.md#amanpreet-singh)
- [![](https://img.shields.io/badge/Alex_Wang-2-blue)](https://github.com/wutong8023/Awesome_Commonsense_Knowledge/blob/main/CommonKnow4all/author/README.md#alex-wang)
- [![](https://img.shields.io/badge/Wen_tau_Yih-2-blue)](https://github.com/wutong8023/Awesome_Commonsense_Knowledge/blob/main/CommonKnow4all/author/README.md#wen-tau-yih)
- [![](https://img.shields.io/badge/Andrew_S._Gordon-2-blue)](https://github.com/wutong8023/Awesome_Commonsense_Knowledge/blob/main/CommonKnow4all/author/README.md#andrew-s.-gordon)
- [![](https://img.shields.io/badge/Noah_A._Smith-2-blue)](https://github.com/wutong8023/Awesome_Commonsense_Knowledge/blob/main/CommonKnow4all/author/README.md#noah-a.-smith)
- [![](https://img.shields.io/badge/Chandra_Bhagavatula-2-blue)](https://github.com/wutong8023/Awesome_Commonsense_Knowledge/blob/main/CommonKnow4all/author/README.md#chandra-bhagavatula)
- [![](https://img.shields.io/badge/Emily_Allaway-2-blue)](https://github.com/wutong8023/Awesome_Commonsense_Knowledge/blob/main/CommonKnow4all/author/README.md#emily-allaway)
- [![](https://img.shields.io/badge/Erik_Cambria-2-blue)](https://github.com/wutong8023/Awesome_Commonsense_Knowledge/blob/main/CommonKnow4all/author/README.md#erik-cambria)
## Hyperlink 
- [Homepage](https://github.com/wutong8023/Awesome_Commonsense_Knowledge/blob/main/README.md)
-  [Summary](https://github.com/wutong8023/Awesome_Commonsense_Knowledge/blob/main/CommonKnow4all/./)
-  [Application](https://github.com/wutong8023/Awesome_Commonsense_Knowledge/blob/main/CommonKnow4all/application)
-  [Author](https://github.com/wutong8023/Awesome_Commonsense_Knowledge/blob/main/CommonKnow4all/author)
-  [Benchmarks](https://github.com/wutong8023/Awesome_Commonsense_Knowledge/blob/main/CommonKnow4all/benchmark)
-  [Knowledge Construction](https://github.com/wutong8023/Awesome_Commonsense_Knowledge/blob/main/CommonKnow4all/construction)
-  [Contribution](https://github.com/wutong8023/Awesome_Commonsense_Knowledge/blob/main/CommonKnow4all/contribution)
-  [Data Source](https://github.com/wutong8023/Awesome_Commonsense_Knowledge/blob/main/CommonKnow4all/data_source)
-  [Published Time](https://github.com/wutong8023/Awesome_Commonsense_Knowledge/blob/main/CommonKnow4all/time)
-  [Knowledge Utilization](https://github.com/wutong8023/Awesome_Commonsense_Knowledge/blob/main/CommonKnow4all/utilization)
-  [Published Venue](https://github.com/wutong8023/Awesome_Commonsense_Knowledge/blob/main/CommonKnow4all/venue)

## Yejin Choi

- [![](https://img.shields.io/badge/ICLR-2020-blue)](https://openreview.net/forum?id=Byg1v1HKDB)<a href="https://scholar.google.com.hk/scholar?q=Abductive+Commonsense+Reasoning"><img src="https://img.shields.io/badge/-blue.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Abductive Commonsense Reasoning**](https://openreview.net/forum?id=Byg1v1HKDB) , <br> by *Chandra Bhagavatula and
Ronan Le Bras and
Chaitanya Malaviya and
Keisuke Sakaguchi and
Ari Holtzman and
Hannah Rashkin and
Doug Downey and
Wen{-}tau Yih and
Yejin Choi* [[bib]](https://github.com/wutong8023/Awesome_Commonsense_Knowledge/blob/main/./bibtex.bib#L303-L317) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Commonsense_Knowledge/blob/main/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```BhagavatulaBMSH20```
- [![](https://img.shields.io/badge/AAAI-2019-blue)](https://doi.org/10.1609/aaai.v33i01.33013027)<a href="https://scholar.google.com.hk/scholar?q=ATOMIC:+An+Atlas+of+Machine+Commonsense+for+If-Then+Reasoning"><img src="https://img.shields.io/badge/-blue.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**ATOMIC: An Atlas of Machine Commonsense for If-Then Reasoning**](https://doi.org/10.1609/aaai.v33i01.33013027) , <br> by *Maarten Sap and
Ronan Le Bras and
Emily Allaway and
Chandra Bhagavatula and
Nicholas Lourie and
Hannah Rashkin and
Brendan Roof and
Noah A. Smith and
Yejin Choi* [[bib]](https://github.com/wutong8023/Awesome_Commonsense_Knowledge/blob/main/./bibtex.bib#L101-L120) <br>```ATOMIC [2019] [309,515 nodes, 877,108 triples], is a commonsense knowledge graph consisting of 877K textual descriptions of inferential knowledge obtained from crowdsourcing. The knowledge graph focuses on if-then relations between events and possible inferences over the events. More specifically, the three types of relation are: “If-Event-Then-Mental-State”, “If-Event-Then-Event”, and “IfEvent-Then-Persona”. The base events were extracted from a variety of corpora
including stories and books.
```</details><details><summary><img src=https://github.com/wutong8023/Awesome_Commonsense_Knowledge/blob/main/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```SapBABLRRSC19```
- [![](https://img.shields.io/badge/EMNLP_IJCNLP-2019-blue)](https://doi.org/10.18653/v1/D19-1454)<a href="https://scholar.google.com.hk/scholar?q=Social+IQa:+Commonsense+Reasoning+about+Social+Interactions"><img src="https://img.shields.io/badge/-blue.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Social IQa: Commonsense Reasoning about Social Interactions**](https://doi.org/10.18653/v1/D19-1454) , <br> by *Maarten Sap and
Hannah Rashkin and
Derek Chen and
Ronan Le Bras and
Yejin Choi* [[bib]](https://github.com/wutong8023/Awesome_Commonsense_Knowledge/blob/main/./bibtex.bib#L284-L299) <br>```AlphaNLI. Similar to ROCStories, AlphaNLI gives two observations as input, and the system needs to choose the most plausible hypothesized event from two choices, which is supposed to have happened between the two observations. The dataset contains
around 170,000 examples.
```</details><details><summary><img src=https://github.com/wutong8023/Awesome_Commonsense_Knowledge/blob/main/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```SapRCBC19```
- [![](https://img.shields.io/badge/ACL-2018-blue)](https://aclanthology.org/P18-1213/)<a href="https://scholar.google.com.hk/scholar?q=Modeling+Naive+Psychology+of+Characters+in+Simple+Commonsense+Stories"><img src="https://img.shields.io/badge/-blue.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Modeling Naive Psychology of Characters in Simple Commonsense Stories**](https://aclanthology.org/P18-1213/) , <br> by *Hannah Rashkin and
Antoine Bosselut and
Maarten Sap and
Kevin Knight and
Yejin Choi* [[bib]](https://github.com/wutong8023/Awesome_Commonsense_Knowledge/blob/main/./bibtex.bib#L220-L237) <br>```Story Commonsense. Story Commonsense [Rashkin et al., 2018a] is a dataset
derived from ROCStories [Mostafazadeh et al., 2016] by annotating the emotions and motivations of the characters. The dataset contains 160,000 examples.
Three classification tasks are involved, namely, the prediction of Maslow’s basic human needs, the prediction of Reiss’ human motives, and the prediction of Plutchik’s eight emotions.
```</details><details><summary><img src=https://github.com/wutong8023/Awesome_Commonsense_Knowledge/blob/main/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```KnightCSRB18```
- [![](https://img.shields.io/badge/ACL-2018-blue)](https://aclanthology.org/P18-1043/)<a href="https://scholar.google.com.hk/scholar?q=Event2Mind:+Commonsense+Inference+on+Events,+Intents,+and+Reactions"><img src="https://img.shields.io/badge/-blue.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Event2Mind: Commonsense Inference on Events, Intents, and Reactions**](https://aclanthology.org/P18-1043/) , <br> by *Hannah Rashkin and
Maarten Sap and
Emily Allaway and
Noah A. Smith and
Yejin Choi* [[bib]](https://github.com/wutong8023/Awesome_Commonsense_Knowledge/blob/main/./bibtex.bib#L244-L260) <br>```Event2Mind, Event2Mind contains around 57,000 annotations of intents and reactions for around 25,000 events extracted from various corpora including ROCStories [Mostafazadeh et al., 2016]. Given an event with one or two participants, the system is supposed to predict the intents and reactions of the primary participant, and the reactions of the other participant.
```</details><details><summary><img src=https://github.com/wutong8023/Awesome_Commonsense_Knowledge/blob/main/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```SmithCSRA18```
- [![](https://img.shields.io/badge/EMNLP-2018-blue)](https://doi.org/10.18653/v1/d18-1009)<a href="https://scholar.google.com.hk/scholar?q=SWAG:+A+Large-Scale+Adversarial+Dataset+for+Grounded+Commonsense+Inference"><img src="https://img.shields.io/badge/-blue.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**SWAG: A Large-Scale Adversarial Dataset for Grounded Commonsense
Inference**](https://doi.org/10.18653/v1/d18-1009) , <br> by *Rowan Zellers and
Yonatan Bisk and
Roy Schwartz and
Yejin Choi* [[bib]](https://github.com/wutong8023/Awesome_Commonsense_Knowledge/blob/main/./bibtex.bib#L264-L279) <br>```SWAG, Situations with Adversarial Generations is a dataset with 113,000 examples, where each example has a beginning sentence followed by four different endings. The system is supposed to choose the most plausible ending from the four choices. The examples of the dataset were filtered adversarially to ensure the problem cannot be solved by simple and
straightforward approaches.
```</details><details><summary><img src=https://github.com/wutong8023/Awesome_Commonsense_Knowledge/blob/main/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```ZellersBSC18```
## Hannah Rashkin

- [![](https://img.shields.io/badge/ICLR-2020-blue)](https://openreview.net/forum?id=Byg1v1HKDB)<a href="https://scholar.google.com.hk/scholar?q=Abductive+Commonsense+Reasoning"><img src="https://img.shields.io/badge/-blue.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Abductive Commonsense Reasoning**](https://openreview.net/forum?id=Byg1v1HKDB) , <br> by *Chandra Bhagavatula and
Ronan Le Bras and
Chaitanya Malaviya and
Keisuke Sakaguchi and
Ari Holtzman and
Hannah Rashkin and
Doug Downey and
Wen{-}tau Yih and
Yejin Choi* [[bib]](https://github.com/wutong8023/Awesome_Commonsense_Knowledge/blob/main/./bibtex.bib#L303-L317) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Commonsense_Knowledge/blob/main/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```BhagavatulaBMSH20```
- [![](https://img.shields.io/badge/AAAI-2019-blue)](https://doi.org/10.1609/aaai.v33i01.33013027)<a href="https://scholar.google.com.hk/scholar?q=ATOMIC:+An+Atlas+of+Machine+Commonsense+for+If-Then+Reasoning"><img src="https://img.shields.io/badge/-blue.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**ATOMIC: An Atlas of Machine Commonsense for If-Then Reasoning**](https://doi.org/10.1609/aaai.v33i01.33013027) , <br> by *Maarten Sap and
Ronan Le Bras and
Emily Allaway and
Chandra Bhagavatula and
Nicholas Lourie and
Hannah Rashkin and
Brendan Roof and
Noah A. Smith and
Yejin Choi* [[bib]](https://github.com/wutong8023/Awesome_Commonsense_Knowledge/blob/main/./bibtex.bib#L101-L120) <br>```ATOMIC [2019] [309,515 nodes, 877,108 triples], is a commonsense knowledge graph consisting of 877K textual descriptions of inferential knowledge obtained from crowdsourcing. The knowledge graph focuses on if-then relations between events and possible inferences over the events. More specifically, the three types of relation are: “If-Event-Then-Mental-State”, “If-Event-Then-Event”, and “IfEvent-Then-Persona”. The base events were extracted from a variety of corpora
including stories and books.
```</details><details><summary><img src=https://github.com/wutong8023/Awesome_Commonsense_Knowledge/blob/main/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```SapBABLRRSC19```
- [![](https://img.shields.io/badge/EMNLP_IJCNLP-2019-blue)](https://doi.org/10.18653/v1/D19-1454)<a href="https://scholar.google.com.hk/scholar?q=Social+IQa:+Commonsense+Reasoning+about+Social+Interactions"><img src="https://img.shields.io/badge/-blue.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Social IQa: Commonsense Reasoning about Social Interactions**](https://doi.org/10.18653/v1/D19-1454) , <br> by *Maarten Sap and
Hannah Rashkin and
Derek Chen and
Ronan Le Bras and
Yejin Choi* [[bib]](https://github.com/wutong8023/Awesome_Commonsense_Knowledge/blob/main/./bibtex.bib#L284-L299) <br>```AlphaNLI. Similar to ROCStories, AlphaNLI gives two observations as input, and the system needs to choose the most plausible hypothesized event from two choices, which is supposed to have happened between the two observations. The dataset contains
around 170,000 examples.
```</details><details><summary><img src=https://github.com/wutong8023/Awesome_Commonsense_Knowledge/blob/main/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```SapRCBC19```
- [![](https://img.shields.io/badge/ACL-2018-blue)](https://aclanthology.org/P18-1213/)<a href="https://scholar.google.com.hk/scholar?q=Modeling+Naive+Psychology+of+Characters+in+Simple+Commonsense+Stories"><img src="https://img.shields.io/badge/-blue.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Modeling Naive Psychology of Characters in Simple Commonsense Stories**](https://aclanthology.org/P18-1213/) , <br> by *Hannah Rashkin and
Antoine Bosselut and
Maarten Sap and
Kevin Knight and
Yejin Choi* [[bib]](https://github.com/wutong8023/Awesome_Commonsense_Knowledge/blob/main/./bibtex.bib#L220-L237) <br>```Story Commonsense. Story Commonsense [Rashkin et al., 2018a] is a dataset
derived from ROCStories [Mostafazadeh et al., 2016] by annotating the emotions and motivations of the characters. The dataset contains 160,000 examples.
Three classification tasks are involved, namely, the prediction of Maslow’s basic human needs, the prediction of Reiss’ human motives, and the prediction of Plutchik’s eight emotions.
```</details><details><summary><img src=https://github.com/wutong8023/Awesome_Commonsense_Knowledge/blob/main/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```KnightCSRB18```
- [![](https://img.shields.io/badge/ACL-2018-blue)](https://aclanthology.org/P18-1043/)<a href="https://scholar.google.com.hk/scholar?q=Event2Mind:+Commonsense+Inference+on+Events,+Intents,+and+Reactions"><img src="https://img.shields.io/badge/-blue.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Event2Mind: Commonsense Inference on Events, Intents, and Reactions**](https://aclanthology.org/P18-1043/) , <br> by *Hannah Rashkin and
Maarten Sap and
Emily Allaway and
Noah A. Smith and
Yejin Choi* [[bib]](https://github.com/wutong8023/Awesome_Commonsense_Knowledge/blob/main/./bibtex.bib#L244-L260) <br>```Event2Mind, Event2Mind contains around 57,000 annotations of intents and reactions for around 25,000 events extracted from various corpora including ROCStories [Mostafazadeh et al., 2016]. Given an event with one or two participants, the system is supposed to predict the intents and reactions of the primary participant, and the reactions of the other participant.
```</details><details><summary><img src=https://github.com/wutong8023/Awesome_Commonsense_Knowledge/blob/main/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```SmithCSRA18```
## Maarten Sap

- [![](https://img.shields.io/badge/AAAI-2019-blue)](https://doi.org/10.1609/aaai.v33i01.33013027)<a href="https://scholar.google.com.hk/scholar?q=ATOMIC:+An+Atlas+of+Machine+Commonsense+for+If-Then+Reasoning"><img src="https://img.shields.io/badge/-blue.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**ATOMIC: An Atlas of Machine Commonsense for If-Then Reasoning**](https://doi.org/10.1609/aaai.v33i01.33013027) , <br> by *Maarten Sap and
Ronan Le Bras and
Emily Allaway and
Chandra Bhagavatula and
Nicholas Lourie and
Hannah Rashkin and
Brendan Roof and
Noah A. Smith and
Yejin Choi* [[bib]](https://github.com/wutong8023/Awesome_Commonsense_Knowledge/blob/main/./bibtex.bib#L101-L120) <br>```ATOMIC [2019] [309,515 nodes, 877,108 triples], is a commonsense knowledge graph consisting of 877K textual descriptions of inferential knowledge obtained from crowdsourcing. The knowledge graph focuses on if-then relations between events and possible inferences over the events. More specifically, the three types of relation are: “If-Event-Then-Mental-State”, “If-Event-Then-Event”, and “IfEvent-Then-Persona”. The base events were extracted from a variety of corpora
including stories and books.
```</details><details><summary><img src=https://github.com/wutong8023/Awesome_Commonsense_Knowledge/blob/main/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```SapBABLRRSC19```
- [![](https://img.shields.io/badge/EMNLP_IJCNLP-2019-blue)](https://doi.org/10.18653/v1/D19-1454)<a href="https://scholar.google.com.hk/scholar?q=Social+IQa:+Commonsense+Reasoning+about+Social+Interactions"><img src="https://img.shields.io/badge/-blue.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Social IQa: Commonsense Reasoning about Social Interactions**](https://doi.org/10.18653/v1/D19-1454) , <br> by *Maarten Sap and
Hannah Rashkin and
Derek Chen and
Ronan Le Bras and
Yejin Choi* [[bib]](https://github.com/wutong8023/Awesome_Commonsense_Knowledge/blob/main/./bibtex.bib#L284-L299) <br>```AlphaNLI. Similar to ROCStories, AlphaNLI gives two observations as input, and the system needs to choose the most plausible hypothesized event from two choices, which is supposed to have happened between the two observations. The dataset contains
around 170,000 examples.
```</details><details><summary><img src=https://github.com/wutong8023/Awesome_Commonsense_Knowledge/blob/main/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```SapRCBC19```
- [![](https://img.shields.io/badge/ACL-2018-blue)](https://aclanthology.org/P18-1213/)<a href="https://scholar.google.com.hk/scholar?q=Modeling+Naive+Psychology+of+Characters+in+Simple+Commonsense+Stories"><img src="https://img.shields.io/badge/-blue.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Modeling Naive Psychology of Characters in Simple Commonsense Stories**](https://aclanthology.org/P18-1213/) , <br> by *Hannah Rashkin and
Antoine Bosselut and
Maarten Sap and
Kevin Knight and
Yejin Choi* [[bib]](https://github.com/wutong8023/Awesome_Commonsense_Knowledge/blob/main/./bibtex.bib#L220-L237) <br>```Story Commonsense. Story Commonsense [Rashkin et al., 2018a] is a dataset
derived from ROCStories [Mostafazadeh et al., 2016] by annotating the emotions and motivations of the characters. The dataset contains 160,000 examples.
Three classification tasks are involved, namely, the prediction of Maslow’s basic human needs, the prediction of Reiss’ human motives, and the prediction of Plutchik’s eight emotions.
```</details><details><summary><img src=https://github.com/wutong8023/Awesome_Commonsense_Knowledge/blob/main/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```KnightCSRB18```
- [![](https://img.shields.io/badge/ACL-2018-blue)](https://aclanthology.org/P18-1043/)<a href="https://scholar.google.com.hk/scholar?q=Event2Mind:+Commonsense+Inference+on+Events,+Intents,+and+Reactions"><img src="https://img.shields.io/badge/-blue.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Event2Mind: Commonsense Inference on Events, Intents, and Reactions**](https://aclanthology.org/P18-1043/) , <br> by *Hannah Rashkin and
Maarten Sap and
Emily Allaway and
Noah A. Smith and
Yejin Choi* [[bib]](https://github.com/wutong8023/Awesome_Commonsense_Knowledge/blob/main/./bibtex.bib#L244-L260) <br>```Event2Mind, Event2Mind contains around 57,000 annotations of intents and reactions for around 25,000 events extracted from various corpora including ROCStories [Mostafazadeh et al., 2016]. Given an event with one or two participants, the system is supposed to predict the intents and reactions of the primary participant, and the reactions of the other participant.
```</details><details><summary><img src=https://github.com/wutong8023/Awesome_Commonsense_Knowledge/blob/main/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```SmithCSRA18```
## Xiang Ren

- [![](https://img.shields.io/badge/EMNLP_Findings-2021-blue)](https://doi.org/10.18653/v1/2021.findings-emnlp.349)<a href="https://scholar.google.com.hk/scholar?q=Probing+Commonsense+Explanation+in+Dialogue+Response+Generation"><img src="https://img.shields.io/badge/-blue.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Probing Commonsense Explanation in Dialogue Response Generation**](https://doi.org/10.18653/v1/2021.findings-emnlp.349) , <br> by *Pei Zhou and
Pegah Jandaghi and
Hyundong Cho and
Bill Yuchen Lin and
Jay Pujara and
Xiang Ren* [[bib]](https://github.com/wutong8023/Awesome_Commonsense_Knowledge/blob/main/./bibtex.bib#L463-L480) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Commonsense_Knowledge/blob/main/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```ZhouJCLPR21```
- [![](https://img.shields.io/badge/SIGDIAL-2021-blue)](https://aclanthology.org/2021.sigdial-1.13)<a href="https://scholar.google.com.hk/scholar?q=Commonsense-Focused+Dialogues+for+Response+Generation:+An+Empirical+Study"><img src="https://img.shields.io/badge/-blue.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Commonsense-Focused Dialogues for Response Generation: An Empirical
Study**](https://aclanthology.org/2021.sigdial-1.13) , <br> by *Pei Zhou and
Karthik Gopalakrishnan and
Behnam Hedayatnia and
Seokhwan Kim and
Jay Pujara and
Xiang Ren and
Yang Liu and
Dilek Hakkani{-}T{\"{u}}r* [[bib]](https://github.com/wutong8023/Awesome_Commonsense_Knowledge/blob/main/./bibtex.bib#L482-L498) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Commonsense_Knowledge/blob/main/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```ZhouGHKPRLH21```
- [![](https://img.shields.io/badge/ACL-2021-blue)](https://doi.org/10.18653/v1/2021.acl-long.102)<a href="https://scholar.google.com.hk/scholar?q=Common+Sense+Beyond+English:+Evaluating+and+Improving+Multilingual+Language+Models+for+Commonsense+Reasoning"><img src="https://img.shields.io/badge/-blue.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Common Sense Beyond English: Evaluating and Improving Multilingual
Language Models for Commonsense Reasoning**](https://doi.org/10.18653/v1/2021.acl-long.102) , <br> by *Bill Yuchen Lin and
Seyeon Lee and
Xiaoyang Qiao and
Xiang Ren* [[bib]](https://github.com/wutong8023/Awesome_Commonsense_Knowledge/blob/main/./bibtex.bib#L535-L549) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Commonsense_Knowledge/blob/main/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```LinLQ020```
## Ronan Le Bras

- [![](https://img.shields.io/badge/ICLR-2020-blue)](https://openreview.net/forum?id=Byg1v1HKDB)<a href="https://scholar.google.com.hk/scholar?q=Abductive+Commonsense+Reasoning"><img src="https://img.shields.io/badge/-blue.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Abductive Commonsense Reasoning**](https://openreview.net/forum?id=Byg1v1HKDB) , <br> by *Chandra Bhagavatula and
Ronan Le Bras and
Chaitanya Malaviya and
Keisuke Sakaguchi and
Ari Holtzman and
Hannah Rashkin and
Doug Downey and
Wen{-}tau Yih and
Yejin Choi* [[bib]](https://github.com/wutong8023/Awesome_Commonsense_Knowledge/blob/main/./bibtex.bib#L303-L317) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Commonsense_Knowledge/blob/main/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```BhagavatulaBMSH20```
- [![](https://img.shields.io/badge/AAAI-2019-blue)](https://doi.org/10.1609/aaai.v33i01.33013027)<a href="https://scholar.google.com.hk/scholar?q=ATOMIC:+An+Atlas+of+Machine+Commonsense+for+If-Then+Reasoning"><img src="https://img.shields.io/badge/-blue.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**ATOMIC: An Atlas of Machine Commonsense for If-Then Reasoning**](https://doi.org/10.1609/aaai.v33i01.33013027) , <br> by *Maarten Sap and
Ronan Le Bras and
Emily Allaway and
Chandra Bhagavatula and
Nicholas Lourie and
Hannah Rashkin and
Brendan Roof and
Noah A. Smith and
Yejin Choi* [[bib]](https://github.com/wutong8023/Awesome_Commonsense_Knowledge/blob/main/./bibtex.bib#L101-L120) <br>```ATOMIC [2019] [309,515 nodes, 877,108 triples], is a commonsense knowledge graph consisting of 877K textual descriptions of inferential knowledge obtained from crowdsourcing. The knowledge graph focuses on if-then relations between events and possible inferences over the events. More specifically, the three types of relation are: “If-Event-Then-Mental-State”, “If-Event-Then-Event”, and “IfEvent-Then-Persona”. The base events were extracted from a variety of corpora
including stories and books.
```</details><details><summary><img src=https://github.com/wutong8023/Awesome_Commonsense_Knowledge/blob/main/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```SapBABLRRSC19```
- [![](https://img.shields.io/badge/EMNLP_IJCNLP-2019-blue)](https://doi.org/10.18653/v1/D19-1454)<a href="https://scholar.google.com.hk/scholar?q=Social+IQa:+Commonsense+Reasoning+about+Social+Interactions"><img src="https://img.shields.io/badge/-blue.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Social IQa: Commonsense Reasoning about Social Interactions**](https://doi.org/10.18653/v1/D19-1454) , <br> by *Maarten Sap and
Hannah Rashkin and
Derek Chen and
Ronan Le Bras and
Yejin Choi* [[bib]](https://github.com/wutong8023/Awesome_Commonsense_Knowledge/blob/main/./bibtex.bib#L284-L299) <br>```AlphaNLI. Similar to ROCStories, AlphaNLI gives two observations as input, and the system needs to choose the most plausible hypothesized event from two choices, which is supposed to have happened between the two observations. The dataset contains
around 170,000 examples.
```</details><details><summary><img src=https://github.com/wutong8023/Awesome_Commonsense_Knowledge/blob/main/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```SapRCBC19```
## Jay Pujara

- [![](https://img.shields.io/badge/EMNLP_Findings-2021-blue)](https://doi.org/10.18653/v1/2021.findings-emnlp.349)<a href="https://scholar.google.com.hk/scholar?q=Probing+Commonsense+Explanation+in+Dialogue+Response+Generation"><img src="https://img.shields.io/badge/-blue.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Probing Commonsense Explanation in Dialogue Response Generation**](https://doi.org/10.18653/v1/2021.findings-emnlp.349) , <br> by *Pei Zhou and
Pegah Jandaghi and
Hyundong Cho and
Bill Yuchen Lin and
Jay Pujara and
Xiang Ren* [[bib]](https://github.com/wutong8023/Awesome_Commonsense_Knowledge/blob/main/./bibtex.bib#L463-L480) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Commonsense_Knowledge/blob/main/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```ZhouJCLPR21```
- [![](https://img.shields.io/badge/SIGDIAL-2021-blue)](https://aclanthology.org/2021.sigdial-1.13)<a href="https://scholar.google.com.hk/scholar?q=Commonsense-Focused+Dialogues+for+Response+Generation:+An+Empirical+Study"><img src="https://img.shields.io/badge/-blue.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Commonsense-Focused Dialogues for Response Generation: An Empirical
Study**](https://aclanthology.org/2021.sigdial-1.13) , <br> by *Pei Zhou and
Karthik Gopalakrishnan and
Behnam Hedayatnia and
Seokhwan Kim and
Jay Pujara and
Xiang Ren and
Yang Liu and
Dilek Hakkani{-}T{\"{u}}r* [[bib]](https://github.com/wutong8023/Awesome_Commonsense_Knowledge/blob/main/./bibtex.bib#L482-L498) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Commonsense_Knowledge/blob/main/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```ZhouGHKPRLH21```
## Bill Yuchen Lin

- [![](https://img.shields.io/badge/EMNLP_Findings-2021-blue)](https://doi.org/10.18653/v1/2021.findings-emnlp.349)<a href="https://scholar.google.com.hk/scholar?q=Probing+Commonsense+Explanation+in+Dialogue+Response+Generation"><img src="https://img.shields.io/badge/-blue.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Probing Commonsense Explanation in Dialogue Response Generation**](https://doi.org/10.18653/v1/2021.findings-emnlp.349) , <br> by *Pei Zhou and
Pegah Jandaghi and
Hyundong Cho and
Bill Yuchen Lin and
Jay Pujara and
Xiang Ren* [[bib]](https://github.com/wutong8023/Awesome_Commonsense_Knowledge/blob/main/./bibtex.bib#L463-L480) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Commonsense_Knowledge/blob/main/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```ZhouJCLPR21```
- [![](https://img.shields.io/badge/ACL-2021-blue)](https://doi.org/10.18653/v1/2021.acl-long.102)<a href="https://scholar.google.com.hk/scholar?q=Common+Sense+Beyond+English:+Evaluating+and+Improving+Multilingual+Language+Models+for+Commonsense+Reasoning"><img src="https://img.shields.io/badge/-blue.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Common Sense Beyond English: Evaluating and Improving Multilingual
Language Models for Commonsense Reasoning**](https://doi.org/10.18653/v1/2021.acl-long.102) , <br> by *Bill Yuchen Lin and
Seyeon Lee and
Xiaoyang Qiao and
Xiang Ren* [[bib]](https://github.com/wutong8023/Awesome_Commonsense_Knowledge/blob/main/./bibtex.bib#L535-L549) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Commonsense_Knowledge/blob/main/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```LinLQ020```
## Pei Zhou

- [![](https://img.shields.io/badge/EMNLP_Findings-2021-blue)](https://doi.org/10.18653/v1/2021.findings-emnlp.349)<a href="https://scholar.google.com.hk/scholar?q=Probing+Commonsense+Explanation+in+Dialogue+Response+Generation"><img src="https://img.shields.io/badge/-blue.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Probing Commonsense Explanation in Dialogue Response Generation**](https://doi.org/10.18653/v1/2021.findings-emnlp.349) , <br> by *Pei Zhou and
Pegah Jandaghi and
Hyundong Cho and
Bill Yuchen Lin and
Jay Pujara and
Xiang Ren* [[bib]](https://github.com/wutong8023/Awesome_Commonsense_Knowledge/blob/main/./bibtex.bib#L463-L480) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Commonsense_Knowledge/blob/main/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```ZhouJCLPR21```
- [![](https://img.shields.io/badge/SIGDIAL-2021-blue)](https://aclanthology.org/2021.sigdial-1.13)<a href="https://scholar.google.com.hk/scholar?q=Commonsense-Focused+Dialogues+for+Response+Generation:+An+Empirical+Study"><img src="https://img.shields.io/badge/-blue.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Commonsense-Focused Dialogues for Response Generation: An Empirical
Study**](https://aclanthology.org/2021.sigdial-1.13) , <br> by *Pei Zhou and
Karthik Gopalakrishnan and
Behnam Hedayatnia and
Seokhwan Kim and
Jay Pujara and
Xiang Ren and
Yang Liu and
Dilek Hakkani{-}T{\"{u}}r* [[bib]](https://github.com/wutong8023/Awesome_Commonsense_Knowledge/blob/main/./bibtex.bib#L482-L498) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Commonsense_Knowledge/blob/main/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```ZhouGHKPRLH21```
## Minlie Huang

- [![](https://img.shields.io/badge/AAAI-2018-blue)](https://ojs.aaai.org/index.php/AAAI/article/view/11923)<a href="https://scholar.google.com.hk/scholar?q=Augmenting+End-to-End+Dialogue+Systems+With+Commonsense+Knowledge"><img src="https://img.shields.io/badge/-blue.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Augmenting End-to-End Dialogue Systems With Commonsense Knowledge**](https://ojs.aaai.org/index.php/AAAI/article/view/11923) , <br> by *Tom Young and
Erik Cambria and
Iti Chaturvedi and
Hao Zhou and
Subham Biswas and
Minlie Huang* [[bib]](https://github.com/wutong8023/Awesome_Commonsense_Knowledge/blob/main/./bibtex.bib#L384-L400) <br>```Retrieval-based Dialogue System.
```</details><details><summary><img src=https://github.com/wutong8023/Awesome_Commonsense_Knowledge/blob/main/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```YoungCCZBH18```
- [![](https://img.shields.io/badge/IJCAI-2018-blue)](https://doi.org/10.24963/ijcai.2018/643)<a href="https://scholar.google.com.hk/scholar?q=Commonsense+Knowledge+Aware+Conversation+Generation+with+Graph+Attention"><img src="https://img.shields.io/badge/-blue.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Commonsense Knowledge Aware Conversation Generation with Graph Attention**](https://doi.org/10.24963/ijcai.2018/643) , <br> by *Hao Zhou and
Tom Young and
Minlie Huang and
Haizhou Zhao and
Jingfang Xu and
Xiaoyan Zhu* [[bib]](https://github.com/wutong8023/Awesome_Commonsense_Knowledge/blob/main/./bibtex.bib#L425-L441) <br>```Template-based Dialogue System, 
```</details><details><summary><img src=https://github.com/wutong8023/Awesome_Commonsense_Knowledge/blob/main/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```ZhouYHZXZ18```
## Hao Zhou

- [![](https://img.shields.io/badge/AAAI-2018-blue)](https://ojs.aaai.org/index.php/AAAI/article/view/11923)<a href="https://scholar.google.com.hk/scholar?q=Augmenting+End-to-End+Dialogue+Systems+With+Commonsense+Knowledge"><img src="https://img.shields.io/badge/-blue.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Augmenting End-to-End Dialogue Systems With Commonsense Knowledge**](https://ojs.aaai.org/index.php/AAAI/article/view/11923) , <br> by *Tom Young and
Erik Cambria and
Iti Chaturvedi and
Hao Zhou and
Subham Biswas and
Minlie Huang* [[bib]](https://github.com/wutong8023/Awesome_Commonsense_Knowledge/blob/main/./bibtex.bib#L384-L400) <br>```Retrieval-based Dialogue System.
```</details><details><summary><img src=https://github.com/wutong8023/Awesome_Commonsense_Knowledge/blob/main/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```YoungCCZBH18```
- [![](https://img.shields.io/badge/IJCAI-2018-blue)](https://doi.org/10.24963/ijcai.2018/643)<a href="https://scholar.google.com.hk/scholar?q=Commonsense+Knowledge+Aware+Conversation+Generation+with+Graph+Attention"><img src="https://img.shields.io/badge/-blue.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Commonsense Knowledge Aware Conversation Generation with Graph Attention**](https://doi.org/10.24963/ijcai.2018/643) , <br> by *Hao Zhou and
Tom Young and
Minlie Huang and
Haizhou Zhao and
Jingfang Xu and
Xiaoyan Zhu* [[bib]](https://github.com/wutong8023/Awesome_Commonsense_Knowledge/blob/main/./bibtex.bib#L425-L441) <br>```Template-based Dialogue System, 
```</details><details><summary><img src=https://github.com/wutong8023/Awesome_Commonsense_Knowledge/blob/main/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```ZhouYHZXZ18```
## Tom Young

- [![](https://img.shields.io/badge/AAAI-2018-blue)](https://ojs.aaai.org/index.php/AAAI/article/view/11923)<a href="https://scholar.google.com.hk/scholar?q=Augmenting+End-to-End+Dialogue+Systems+With+Commonsense+Knowledge"><img src="https://img.shields.io/badge/-blue.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Augmenting End-to-End Dialogue Systems With Commonsense Knowledge**](https://ojs.aaai.org/index.php/AAAI/article/view/11923) , <br> by *Tom Young and
Erik Cambria and
Iti Chaturvedi and
Hao Zhou and
Subham Biswas and
Minlie Huang* [[bib]](https://github.com/wutong8023/Awesome_Commonsense_Knowledge/blob/main/./bibtex.bib#L384-L400) <br>```Retrieval-based Dialogue System.
```</details><details><summary><img src=https://github.com/wutong8023/Awesome_Commonsense_Knowledge/blob/main/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```YoungCCZBH18```
- [![](https://img.shields.io/badge/IJCAI-2018-blue)](https://doi.org/10.24963/ijcai.2018/643)<a href="https://scholar.google.com.hk/scholar?q=Commonsense+Knowledge+Aware+Conversation+Generation+with+Graph+Attention"><img src="https://img.shields.io/badge/-blue.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Commonsense Knowledge Aware Conversation Generation with Graph Attention**](https://doi.org/10.24963/ijcai.2018/643) , <br> by *Hao Zhou and
Tom Young and
Minlie Huang and
Haizhou Zhao and
Jingfang Xu and
Xiaoyan Zhu* [[bib]](https://github.com/wutong8023/Awesome_Commonsense_Knowledge/blob/main/./bibtex.bib#L425-L441) <br>```Template-based Dialogue System, 
```</details><details><summary><img src=https://github.com/wutong8023/Awesome_Commonsense_Knowledge/blob/main/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```ZhouYHZXZ18```
## Samuel R. Bowman

- [![](https://img.shields.io/badge/ICLR-2019-blue)](https://openreview.net/forum?id=rJ4km2R5t7)<a href="https://scholar.google.com.hk/scholar?q=GLUE:+A+Multi-Task+Benchmark+and+Analysis+Platform+for+Natural+Language+Understanding"><img src="https://img.shields.io/badge/-blue.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**GLUE: A Multi-Task Benchmark and Analysis Platform for Natural
Language Understanding**](https://openreview.net/forum?id=rJ4km2R5t7) , <br> by *Alex Wang and
Amanpreet Singh and
Julian Michael and
Felix Hill and
Omer Levy and
Samuel R. Bowman* [[bib]](https://github.com/wutong8023/Awesome_Commonsense_Knowledge/blob/main/./bibtex.bib#L322-L337) <br>```GLUE. The GLUE benchmark is a popular collection of resources for the evaluation of natural language processing systems, which contains multiple tasks that would require systems to have the ability of commonsense reasoning, e.g., natural language inference, textual entailment, and question answering.
```</details><details><summary><img src=https://github.com/wutong8023/Awesome_Commonsense_Knowledge/blob/main/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```WangSMHLB19```
- [![](https://img.shields.io/badge/NeurIPS-2019-blue)](https://proceedings.neurips.cc/paper/2019/hash/4496bf24afe7fab6f046bf4923da8de6-Abstract.html)<a href="https://scholar.google.com.hk/scholar?q=SuperGLUE:+A+Stickier+Benchmark+for+General-Purpose+Language+Understanding+Systems"><img src="https://img.shields.io/badge/-blue.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding
Systems**](https://proceedings.neurips.cc/paper/2019/hash/4496bf24afe7fab6f046bf4923da8de6-Abstract.html) , <br> by *Alex Wang and
Yada Pruksachatkun and
Nikita Nangia and
Amanpreet Singh and
Julian Michael and
Felix Hill and
Omer Levy and
Samuel R. Bowman* [[bib]](https://github.com/wutong8023/Awesome_Commonsense_Knowledge/blob/main/./bibtex.bib#L341-L360) <br>```SuperGLUE. The SuperGLUE benchmark improves the GLUE benchmark by incorporating more diﬃcult natural language understanding tasks, to accommodate the increasingly powerful natural language processing systems in recent years.
```</details><details><summary><img src=https://github.com/wutong8023/Awesome_Commonsense_Knowledge/blob/main/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```WangPNSMHLB19```
## Omer Levy

- [![](https://img.shields.io/badge/ICLR-2019-blue)](https://openreview.net/forum?id=rJ4km2R5t7)<a href="https://scholar.google.com.hk/scholar?q=GLUE:+A+Multi-Task+Benchmark+and+Analysis+Platform+for+Natural+Language+Understanding"><img src="https://img.shields.io/badge/-blue.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**GLUE: A Multi-Task Benchmark and Analysis Platform for Natural
Language Understanding**](https://openreview.net/forum?id=rJ4km2R5t7) , <br> by *Alex Wang and
Amanpreet Singh and
Julian Michael and
Felix Hill and
Omer Levy and
Samuel R. Bowman* [[bib]](https://github.com/wutong8023/Awesome_Commonsense_Knowledge/blob/main/./bibtex.bib#L322-L337) <br>```GLUE. The GLUE benchmark is a popular collection of resources for the evaluation of natural language processing systems, which contains multiple tasks that would require systems to have the ability of commonsense reasoning, e.g., natural language inference, textual entailment, and question answering.
```</details><details><summary><img src=https://github.com/wutong8023/Awesome_Commonsense_Knowledge/blob/main/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```WangSMHLB19```
- [![](https://img.shields.io/badge/NeurIPS-2019-blue)](https://proceedings.neurips.cc/paper/2019/hash/4496bf24afe7fab6f046bf4923da8de6-Abstract.html)<a href="https://scholar.google.com.hk/scholar?q=SuperGLUE:+A+Stickier+Benchmark+for+General-Purpose+Language+Understanding+Systems"><img src="https://img.shields.io/badge/-blue.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding
Systems**](https://proceedings.neurips.cc/paper/2019/hash/4496bf24afe7fab6f046bf4923da8de6-Abstract.html) , <br> by *Alex Wang and
Yada Pruksachatkun and
Nikita Nangia and
Amanpreet Singh and
Julian Michael and
Felix Hill and
Omer Levy and
Samuel R. Bowman* [[bib]](https://github.com/wutong8023/Awesome_Commonsense_Knowledge/blob/main/./bibtex.bib#L341-L360) <br>```SuperGLUE. The SuperGLUE benchmark improves the GLUE benchmark by incorporating more diﬃcult natural language understanding tasks, to accommodate the increasingly powerful natural language processing systems in recent years.
```</details><details><summary><img src=https://github.com/wutong8023/Awesome_Commonsense_Knowledge/blob/main/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```WangPNSMHLB19```
## Felix Hill

- [![](https://img.shields.io/badge/ICLR-2019-blue)](https://openreview.net/forum?id=rJ4km2R5t7)<a href="https://scholar.google.com.hk/scholar?q=GLUE:+A+Multi-Task+Benchmark+and+Analysis+Platform+for+Natural+Language+Understanding"><img src="https://img.shields.io/badge/-blue.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**GLUE: A Multi-Task Benchmark and Analysis Platform for Natural
Language Understanding**](https://openreview.net/forum?id=rJ4km2R5t7) , <br> by *Alex Wang and
Amanpreet Singh and
Julian Michael and
Felix Hill and
Omer Levy and
Samuel R. Bowman* [[bib]](https://github.com/wutong8023/Awesome_Commonsense_Knowledge/blob/main/./bibtex.bib#L322-L337) <br>```GLUE. The GLUE benchmark is a popular collection of resources for the evaluation of natural language processing systems, which contains multiple tasks that would require systems to have the ability of commonsense reasoning, e.g., natural language inference, textual entailment, and question answering.
```</details><details><summary><img src=https://github.com/wutong8023/Awesome_Commonsense_Knowledge/blob/main/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```WangSMHLB19```
- [![](https://img.shields.io/badge/NeurIPS-2019-blue)](https://proceedings.neurips.cc/paper/2019/hash/4496bf24afe7fab6f046bf4923da8de6-Abstract.html)<a href="https://scholar.google.com.hk/scholar?q=SuperGLUE:+A+Stickier+Benchmark+for+General-Purpose+Language+Understanding+Systems"><img src="https://img.shields.io/badge/-blue.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding
Systems**](https://proceedings.neurips.cc/paper/2019/hash/4496bf24afe7fab6f046bf4923da8de6-Abstract.html) , <br> by *Alex Wang and
Yada Pruksachatkun and
Nikita Nangia and
Amanpreet Singh and
Julian Michael and
Felix Hill and
Omer Levy and
Samuel R. Bowman* [[bib]](https://github.com/wutong8023/Awesome_Commonsense_Knowledge/blob/main/./bibtex.bib#L341-L360) <br>```SuperGLUE. The SuperGLUE benchmark improves the GLUE benchmark by incorporating more diﬃcult natural language understanding tasks, to accommodate the increasingly powerful natural language processing systems in recent years.
```</details><details><summary><img src=https://github.com/wutong8023/Awesome_Commonsense_Knowledge/blob/main/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```WangPNSMHLB19```
## Julian Michael

- [![](https://img.shields.io/badge/ICLR-2019-blue)](https://openreview.net/forum?id=rJ4km2R5t7)<a href="https://scholar.google.com.hk/scholar?q=GLUE:+A+Multi-Task+Benchmark+and+Analysis+Platform+for+Natural+Language+Understanding"><img src="https://img.shields.io/badge/-blue.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**GLUE: A Multi-Task Benchmark and Analysis Platform for Natural
Language Understanding**](https://openreview.net/forum?id=rJ4km2R5t7) , <br> by *Alex Wang and
Amanpreet Singh and
Julian Michael and
Felix Hill and
Omer Levy and
Samuel R. Bowman* [[bib]](https://github.com/wutong8023/Awesome_Commonsense_Knowledge/blob/main/./bibtex.bib#L322-L337) <br>```GLUE. The GLUE benchmark is a popular collection of resources for the evaluation of natural language processing systems, which contains multiple tasks that would require systems to have the ability of commonsense reasoning, e.g., natural language inference, textual entailment, and question answering.
```</details><details><summary><img src=https://github.com/wutong8023/Awesome_Commonsense_Knowledge/blob/main/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```WangSMHLB19```
- [![](https://img.shields.io/badge/NeurIPS-2019-blue)](https://proceedings.neurips.cc/paper/2019/hash/4496bf24afe7fab6f046bf4923da8de6-Abstract.html)<a href="https://scholar.google.com.hk/scholar?q=SuperGLUE:+A+Stickier+Benchmark+for+General-Purpose+Language+Understanding+Systems"><img src="https://img.shields.io/badge/-blue.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding
Systems**](https://proceedings.neurips.cc/paper/2019/hash/4496bf24afe7fab6f046bf4923da8de6-Abstract.html) , <br> by *Alex Wang and
Yada Pruksachatkun and
Nikita Nangia and
Amanpreet Singh and
Julian Michael and
Felix Hill and
Omer Levy and
Samuel R. Bowman* [[bib]](https://github.com/wutong8023/Awesome_Commonsense_Knowledge/blob/main/./bibtex.bib#L341-L360) <br>```SuperGLUE. The SuperGLUE benchmark improves the GLUE benchmark by incorporating more diﬃcult natural language understanding tasks, to accommodate the increasingly powerful natural language processing systems in recent years.
```</details><details><summary><img src=https://github.com/wutong8023/Awesome_Commonsense_Knowledge/blob/main/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```WangPNSMHLB19```
## Amanpreet Singh

- [![](https://img.shields.io/badge/ICLR-2019-blue)](https://openreview.net/forum?id=rJ4km2R5t7)<a href="https://scholar.google.com.hk/scholar?q=GLUE:+A+Multi-Task+Benchmark+and+Analysis+Platform+for+Natural+Language+Understanding"><img src="https://img.shields.io/badge/-blue.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**GLUE: A Multi-Task Benchmark and Analysis Platform for Natural
Language Understanding**](https://openreview.net/forum?id=rJ4km2R5t7) , <br> by *Alex Wang and
Amanpreet Singh and
Julian Michael and
Felix Hill and
Omer Levy and
Samuel R. Bowman* [[bib]](https://github.com/wutong8023/Awesome_Commonsense_Knowledge/blob/main/./bibtex.bib#L322-L337) <br>```GLUE. The GLUE benchmark is a popular collection of resources for the evaluation of natural language processing systems, which contains multiple tasks that would require systems to have the ability of commonsense reasoning, e.g., natural language inference, textual entailment, and question answering.
```</details><details><summary><img src=https://github.com/wutong8023/Awesome_Commonsense_Knowledge/blob/main/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```WangSMHLB19```
- [![](https://img.shields.io/badge/NeurIPS-2019-blue)](https://proceedings.neurips.cc/paper/2019/hash/4496bf24afe7fab6f046bf4923da8de6-Abstract.html)<a href="https://scholar.google.com.hk/scholar?q=SuperGLUE:+A+Stickier+Benchmark+for+General-Purpose+Language+Understanding+Systems"><img src="https://img.shields.io/badge/-blue.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding
Systems**](https://proceedings.neurips.cc/paper/2019/hash/4496bf24afe7fab6f046bf4923da8de6-Abstract.html) , <br> by *Alex Wang and
Yada Pruksachatkun and
Nikita Nangia and
Amanpreet Singh and
Julian Michael and
Felix Hill and
Omer Levy and
Samuel R. Bowman* [[bib]](https://github.com/wutong8023/Awesome_Commonsense_Knowledge/blob/main/./bibtex.bib#L341-L360) <br>```SuperGLUE. The SuperGLUE benchmark improves the GLUE benchmark by incorporating more diﬃcult natural language understanding tasks, to accommodate the increasingly powerful natural language processing systems in recent years.
```</details><details><summary><img src=https://github.com/wutong8023/Awesome_Commonsense_Knowledge/blob/main/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```WangPNSMHLB19```
## Alex Wang

- [![](https://img.shields.io/badge/ICLR-2019-blue)](https://openreview.net/forum?id=rJ4km2R5t7)<a href="https://scholar.google.com.hk/scholar?q=GLUE:+A+Multi-Task+Benchmark+and+Analysis+Platform+for+Natural+Language+Understanding"><img src="https://img.shields.io/badge/-blue.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**GLUE: A Multi-Task Benchmark and Analysis Platform for Natural
Language Understanding**](https://openreview.net/forum?id=rJ4km2R5t7) , <br> by *Alex Wang and
Amanpreet Singh and
Julian Michael and
Felix Hill and
Omer Levy and
Samuel R. Bowman* [[bib]](https://github.com/wutong8023/Awesome_Commonsense_Knowledge/blob/main/./bibtex.bib#L322-L337) <br>```GLUE. The GLUE benchmark is a popular collection of resources for the evaluation of natural language processing systems, which contains multiple tasks that would require systems to have the ability of commonsense reasoning, e.g., natural language inference, textual entailment, and question answering.
```</details><details><summary><img src=https://github.com/wutong8023/Awesome_Commonsense_Knowledge/blob/main/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```WangSMHLB19```
- [![](https://img.shields.io/badge/NeurIPS-2019-blue)](https://proceedings.neurips.cc/paper/2019/hash/4496bf24afe7fab6f046bf4923da8de6-Abstract.html)<a href="https://scholar.google.com.hk/scholar?q=SuperGLUE:+A+Stickier+Benchmark+for+General-Purpose+Language+Understanding+Systems"><img src="https://img.shields.io/badge/-blue.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding
Systems**](https://proceedings.neurips.cc/paper/2019/hash/4496bf24afe7fab6f046bf4923da8de6-Abstract.html) , <br> by *Alex Wang and
Yada Pruksachatkun and
Nikita Nangia and
Amanpreet Singh and
Julian Michael and
Felix Hill and
Omer Levy and
Samuel R. Bowman* [[bib]](https://github.com/wutong8023/Awesome_Commonsense_Knowledge/blob/main/./bibtex.bib#L341-L360) <br>```SuperGLUE. The SuperGLUE benchmark improves the GLUE benchmark by incorporating more diﬃcult natural language understanding tasks, to accommodate the increasingly powerful natural language processing systems in recent years.
```</details><details><summary><img src=https://github.com/wutong8023/Awesome_Commonsense_Knowledge/blob/main/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```WangPNSMHLB19```
## Wen tau Yih

- [![](https://img.shields.io/badge/ICLR-2020-blue)](https://openreview.net/forum?id=Byg1v1HKDB)<a href="https://scholar.google.com.hk/scholar?q=Abductive+Commonsense+Reasoning"><img src="https://img.shields.io/badge/-blue.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Abductive Commonsense Reasoning**](https://openreview.net/forum?id=Byg1v1HKDB) , <br> by *Chandra Bhagavatula and
Ronan Le Bras and
Chaitanya Malaviya and
Keisuke Sakaguchi and
Ari Holtzman and
Hannah Rashkin and
Doug Downey and
Wen{-}tau Yih and
Yejin Choi* [[bib]](https://github.com/wutong8023/Awesome_Commonsense_Knowledge/blob/main/./bibtex.bib#L303-L317) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Commonsense_Knowledge/blob/main/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```BhagavatulaBMSH20```
- [![](https://img.shields.io/badge/AAAI-2018-blue)](https://ojs.aaai.org/index.php/AAAI/article/view/11977)<a href="https://scholar.google.com.hk/scholar?q=A+Knowledge-Grounded+Neural+Conversation+Model"><img src="https://img.shields.io/badge/-blue.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**A Knowledge-Grounded Neural Conversation Model**](https://ojs.aaai.org/index.php/AAAI/article/view/11977) , <br> by *Marjan Ghazvininejad and
Chris Brockett and
Ming{-}Wei Chang and
Bill Dolan and
Jianfeng Gao and
Wen{-}tau Yih and
Michel Galley* [[bib]](https://github.com/wutong8023/Awesome_Commonsense_Knowledge/blob/main/./bibtex.bib#L404-L421) <br>```Retrieval-based Dialogue System, 
```</details><details><summary><img src=https://github.com/wutong8023/Awesome_Commonsense_Knowledge/blob/main/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```GhazvininejadBC18```
## Andrew S. Gordon

- [![](https://img.shields.io/badge/AAAI-2016-blue)](http://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/view/11790)<a href="https://scholar.google.com.hk/scholar?q=Commonsense+Interpretation+of+Triangle+Behavior"><img src="https://img.shields.io/badge/-blue.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Commonsense Interpretation of Triangle Behavior**](http://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/view/11790) , <br> by *Andrew S. Gordon* [[bib]](https://github.com/wutong8023/Awesome_Commonsense_Knowledge/blob/main/./bibtex.bib#L180-L191) <br>```Triangle-COPA. Triangle-COPA is a variation of COPA with 100 examples in the same format but accompanied with videos. The videos show situations where a circle and a triangle interact with each other. The questions asked are more focused on emotions and intentions.
```</details><details><summary><img src=https://github.com/wutong8023/Awesome_Commonsense_Knowledge/blob/main/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```Gordon16```
- [![](https://img.shields.io/badge/AAAI-2011-blue)](http://www.aaai.org/ocs/index.php/SSS/SSS11/paper/view/2418)<a href="https://scholar.google.com.hk/scholar?q=Choice+of+Plausible+Alternatives:+An+Evaluation+of+Commonsense+Causal+Reasoning"><img src="https://img.shields.io/badge/-blue.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Choice of Plausible Alternatives: An Evaluation of Commonsense Causal
Reasoning**](http://www.aaai.org/ocs/index.php/SSS/SSS11/paper/view/2418) , <br> by *Melissa Roemmele and
Cosmin Adrian Bejan and
Andrew S. Gordon* [[bib]](https://github.com/wutong8023/Awesome_Commonsense_Knowledge/blob/main/./bibtex.bib#L162-L175) <br>```COPA, The Choice of Plausible Alternatives involves causal inference between events. The dataset contains 1,000 examples in total, and in each example, an event is given, followed by a question asking
for the correct effect or cause from two options.
```</details><details><summary><img src=https://github.com/wutong8023/Awesome_Commonsense_Knowledge/blob/main/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```RoemmeleBG11```
## Noah A. Smith

- [![](https://img.shields.io/badge/AAAI-2019-blue)](https://doi.org/10.1609/aaai.v33i01.33013027)<a href="https://scholar.google.com.hk/scholar?q=ATOMIC:+An+Atlas+of+Machine+Commonsense+for+If-Then+Reasoning"><img src="https://img.shields.io/badge/-blue.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**ATOMIC: An Atlas of Machine Commonsense for If-Then Reasoning**](https://doi.org/10.1609/aaai.v33i01.33013027) , <br> by *Maarten Sap and
Ronan Le Bras and
Emily Allaway and
Chandra Bhagavatula and
Nicholas Lourie and
Hannah Rashkin and
Brendan Roof and
Noah A. Smith and
Yejin Choi* [[bib]](https://github.com/wutong8023/Awesome_Commonsense_Knowledge/blob/main/./bibtex.bib#L101-L120) <br>```ATOMIC [2019] [309,515 nodes, 877,108 triples], is a commonsense knowledge graph consisting of 877K textual descriptions of inferential knowledge obtained from crowdsourcing. The knowledge graph focuses on if-then relations between events and possible inferences over the events. More specifically, the three types of relation are: “If-Event-Then-Mental-State”, “If-Event-Then-Event”, and “IfEvent-Then-Persona”. The base events were extracted from a variety of corpora
including stories and books.
```</details><details><summary><img src=https://github.com/wutong8023/Awesome_Commonsense_Knowledge/blob/main/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```SapBABLRRSC19```
- [![](https://img.shields.io/badge/ACL-2018-blue)](https://aclanthology.org/P18-1043/)<a href="https://scholar.google.com.hk/scholar?q=Event2Mind:+Commonsense+Inference+on+Events,+Intents,+and+Reactions"><img src="https://img.shields.io/badge/-blue.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Event2Mind: Commonsense Inference on Events, Intents, and Reactions**](https://aclanthology.org/P18-1043/) , <br> by *Hannah Rashkin and
Maarten Sap and
Emily Allaway and
Noah A. Smith and
Yejin Choi* [[bib]](https://github.com/wutong8023/Awesome_Commonsense_Knowledge/blob/main/./bibtex.bib#L244-L260) <br>```Event2Mind, Event2Mind contains around 57,000 annotations of intents and reactions for around 25,000 events extracted from various corpora including ROCStories [Mostafazadeh et al., 2016]. Given an event with one or two participants, the system is supposed to predict the intents and reactions of the primary participant, and the reactions of the other participant.
```</details><details><summary><img src=https://github.com/wutong8023/Awesome_Commonsense_Knowledge/blob/main/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```SmithCSRA18```
## Chandra Bhagavatula

- [![](https://img.shields.io/badge/ICLR-2020-blue)](https://openreview.net/forum?id=Byg1v1HKDB)<a href="https://scholar.google.com.hk/scholar?q=Abductive+Commonsense+Reasoning"><img src="https://img.shields.io/badge/-blue.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Abductive Commonsense Reasoning**](https://openreview.net/forum?id=Byg1v1HKDB) , <br> by *Chandra Bhagavatula and
Ronan Le Bras and
Chaitanya Malaviya and
Keisuke Sakaguchi and
Ari Holtzman and
Hannah Rashkin and
Doug Downey and
Wen{-}tau Yih and
Yejin Choi* [[bib]](https://github.com/wutong8023/Awesome_Commonsense_Knowledge/blob/main/./bibtex.bib#L303-L317) <br></details><details><summary><img src=https://github.com/wutong8023/Awesome_Commonsense_Knowledge/blob/main/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```BhagavatulaBMSH20```
- [![](https://img.shields.io/badge/AAAI-2019-blue)](https://doi.org/10.1609/aaai.v33i01.33013027)<a href="https://scholar.google.com.hk/scholar?q=ATOMIC:+An+Atlas+of+Machine+Commonsense+for+If-Then+Reasoning"><img src="https://img.shields.io/badge/-blue.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**ATOMIC: An Atlas of Machine Commonsense for If-Then Reasoning**](https://doi.org/10.1609/aaai.v33i01.33013027) , <br> by *Maarten Sap and
Ronan Le Bras and
Emily Allaway and
Chandra Bhagavatula and
Nicholas Lourie and
Hannah Rashkin and
Brendan Roof and
Noah A. Smith and
Yejin Choi* [[bib]](https://github.com/wutong8023/Awesome_Commonsense_Knowledge/blob/main/./bibtex.bib#L101-L120) <br>```ATOMIC [2019] [309,515 nodes, 877,108 triples], is a commonsense knowledge graph consisting of 877K textual descriptions of inferential knowledge obtained from crowdsourcing. The knowledge graph focuses on if-then relations between events and possible inferences over the events. More specifically, the three types of relation are: “If-Event-Then-Mental-State”, “If-Event-Then-Event”, and “IfEvent-Then-Persona”. The base events were extracted from a variety of corpora
including stories and books.
```</details><details><summary><img src=https://github.com/wutong8023/Awesome_Commonsense_Knowledge/blob/main/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```SapBABLRRSC19```
## Emily Allaway

- [![](https://img.shields.io/badge/AAAI-2019-blue)](https://doi.org/10.1609/aaai.v33i01.33013027)<a href="https://scholar.google.com.hk/scholar?q=ATOMIC:+An+Atlas+of+Machine+Commonsense+for+If-Then+Reasoning"><img src="https://img.shields.io/badge/-blue.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**ATOMIC: An Atlas of Machine Commonsense for If-Then Reasoning**](https://doi.org/10.1609/aaai.v33i01.33013027) , <br> by *Maarten Sap and
Ronan Le Bras and
Emily Allaway and
Chandra Bhagavatula and
Nicholas Lourie and
Hannah Rashkin and
Brendan Roof and
Noah A. Smith and
Yejin Choi* [[bib]](https://github.com/wutong8023/Awesome_Commonsense_Knowledge/blob/main/./bibtex.bib#L101-L120) <br>```ATOMIC [2019] [309,515 nodes, 877,108 triples], is a commonsense knowledge graph consisting of 877K textual descriptions of inferential knowledge obtained from crowdsourcing. The knowledge graph focuses on if-then relations between events and possible inferences over the events. More specifically, the three types of relation are: “If-Event-Then-Mental-State”, “If-Event-Then-Event”, and “IfEvent-Then-Persona”. The base events were extracted from a variety of corpora
including stories and books.
```</details><details><summary><img src=https://github.com/wutong8023/Awesome_Commonsense_Knowledge/blob/main/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```SapBABLRRSC19```
- [![](https://img.shields.io/badge/ACL-2018-blue)](https://aclanthology.org/P18-1043/)<a href="https://scholar.google.com.hk/scholar?q=Event2Mind:+Commonsense+Inference+on+Events,+Intents,+and+Reactions"><img src="https://img.shields.io/badge/-blue.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Event2Mind: Commonsense Inference on Events, Intents, and Reactions**](https://aclanthology.org/P18-1043/) , <br> by *Hannah Rashkin and
Maarten Sap and
Emily Allaway and
Noah A. Smith and
Yejin Choi* [[bib]](https://github.com/wutong8023/Awesome_Commonsense_Knowledge/blob/main/./bibtex.bib#L244-L260) <br>```Event2Mind, Event2Mind contains around 57,000 annotations of intents and reactions for around 25,000 events extracted from various corpora including ROCStories [Mostafazadeh et al., 2016]. Given an event with one or two participants, the system is supposed to predict the intents and reactions of the primary participant, and the reactions of the other participant.
```</details><details><summary><img src=https://github.com/wutong8023/Awesome_Commonsense_Knowledge/blob/main/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```SmithCSRA18```
## Erik Cambria

- [![](https://img.shields.io/badge/CIKM-2020-blue)](https://doi.org/10.1145/3340531.3412003)<a href="https://scholar.google.com.hk/scholar?q=SenticNet+6:+Ensemble+Application+of+Symbolic+and+Subsymbolic+AI+for+Sentiment+Analysis"><img src="https://img.shields.io/badge/-blue.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**SenticNet 6: Ensemble Application of Symbolic and Subsymbolic AI
for Sentiment Analysis**](https://doi.org/10.1145/3340531.3412003) , <br> by *Erik Cambria and
Yang Li and
Frank Z. Xing and
Soujanya Poria and
Kenneth Kwok* [[bib]](https://github.com/wutong8023/Awesome_Commonsense_Knowledge/blob/main/./bibtex.bib#L65-L81) <br>```SenticNet[2009-2020][200,000 concepts]. Speciﬁcally, semantics deﬁne the denotative information associated with natural language phrases, sentics deﬁne the emotion categorization values, expressed in terms of four aﬀective dimensions associated with these concepts, and polarity is ﬂoating number between −1 and +1. The knowledge base is automatically created from multiple other resources, e.g., WordNet-Affect and OMCS.
```</details><details><summary><img src=https://github.com/wutong8023/Awesome_Commonsense_Knowledge/blob/main/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```CambriaLXPK20```
- [![](https://img.shields.io/badge/AAAI-2018-blue)](https://ojs.aaai.org/index.php/AAAI/article/view/11923)<a href="https://scholar.google.com.hk/scholar?q=Augmenting+End-to-End+Dialogue+Systems+With+Commonsense+Knowledge"><img src="https://img.shields.io/badge/-blue.svg?&logo=google-scholar&logoColor=white" height="18" align="bottom"></a> [**Augmenting End-to-End Dialogue Systems With Commonsense Knowledge**](https://ojs.aaai.org/index.php/AAAI/article/view/11923) , <br> by *Tom Young and
Erik Cambria and
Iti Chaturvedi and
Hao Zhou and
Subham Biswas and
Minlie Huang* [[bib]](https://github.com/wutong8023/Awesome_Commonsense_Knowledge/blob/main/./bibtex.bib#L384-L400) <br>```Retrieval-based Dialogue System.
```</details><details><summary><img src=https://github.com/wutong8023/Awesome_Commonsense_Knowledge/blob/main/scripts/svg/copy_icon.png height="20" align="bottom"></summary><pre>```YoungCCZBH18```